{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d35898-dfc0-4002-86f8-b66b90dd5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3109145f-b9e2-489f-b0e8-eb83dff91ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 32 # 128\n",
    "num_workers = 0 # 4\n",
    "model_name = \"mobilenet\"\n",
    "num_epochs = 1\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_csv_file = \"./image-data/labels-map.csv\"\n",
    "train_csv_file = \"./image-data/train-labels.csv\"\n",
    "valid_csv_file = \"./image-data/valid-labels.csv\"\n",
    "test_csv_file = \"./image-data/test-labels.csv\"\n",
    "label_file = \"./labels/256-common-hangul.txt\"\n",
    "image_dir = \"./image-data/hangul-images\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "625b80be-0c61-4eda-b6b9-cb2c119b5edc",
   "metadata": {},
   "source": [
    "# Seed 설정 함수\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee0f3b0-3524-402d-8cfe-66ce8c783bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분리 함수\n",
    "def split_dataset(csv_path, train_save_path, valid_save_path, test_save_path, test_size, valid_size, random_seed):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path, header=None, names=['path', 'label'])\n",
    "    \n",
    "    # 먼저 전체 데이터셋을 train + validation과 test로 분리\n",
    "    train_valid_df, test_df = train_test_split(df, test_size=test_size, random_state=random_seed, stratify=df['label'])\n",
    "    \n",
    "    # train + validation 데이터셋을 train과 validation으로 분리\n",
    "    train_size = 1 - valid_size\n",
    "    train_df, valid_df = train_test_split(train_valid_df, test_size=valid_size/(train_size + valid_size), random_state=random_seed, stratify=train_valid_df['label'])\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    train_df.to_csv(train_save_path, index=False, header=False)\n",
    "    valid_df.to_csv(valid_save_path, index=False, header=False)\n",
    "    test_df.to_csv(test_save_path, index=False, header=False)    \n",
    "\n",
    "# 데이터셋 클래스\n",
    "class KoreanHandwritingDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, label_file, transform=None):\n",
    "        self.dataset = pd.read_csv(csv_file, header=None, names=['path', 'label'])\n",
    "        self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "        with open(self.label_file, 'r', encoding='utf-8') as f:\n",
    "            hangul_chars = [line.strip() for line in f.readlines()]\n",
    "        self.label_mapping = {char: idx for idx, char in enumerate(hangul_chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, os.path.basename(self.dataset.iloc[idx]['path']))\n",
    "        \n",
    "        # 그레이스케일로 이미지 열기\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "        label = self.label_mapping[label]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "      \n",
    "            \n",
    "# 데이터 변환\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 먼저 이미지를 더 크게 리사이즈\n",
    "    transforms.CenterCrop(224),  # 그 다음 중앙을 224x224 크기로 크롭\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 먼저 이미지를 더 크게 리사이즈\n",
    "    transforms.CenterCrop(224),  # 그 다음 중앙을 224x224 크기로 크롭\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 먼저 이미지를 더 크게 리사이즈\n",
    "    transforms.CenterCrop(224),  # 그 다음 중앙을 224x224 크기로 크롭\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# 데이터셋 및 데이터 로더 준비\n",
    "split_dataset(data_csv_file, train_csv_file, valid_csv_file, test_csv_file, 0.2, 0.1, 42)\n",
    "\n",
    "train_dataset = KoreanHandwritingDataset(train_csv_file, image_dir, label_file, train_transform)\n",
    "valid_dataset = KoreanHandwritingDataset(valid_csv_file, image_dir, label_file, valid_transform)\n",
    "test_dataset = KoreanHandwritingDataset(test_csv_file, image_dir, label_file, test_transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa2351d-2086-4516-bfdf-c2404534244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True, checkpoint_path=None):\n",
    "    # 모델을 선택하고 초기화합니다.\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18 \"\"\"\n",
    "        model = models.resnet18(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet \"\"\"\n",
    "        model = models.alexnet(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        \n",
    "    elif model_name == \"mobilenet\":\n",
    "        \"\"\" Mobilenet_v2 \"\"\"\n",
    "        model = models.mobilenet_v2(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG19_bn \"\"\"\n",
    "        model = models.vgg19_bn(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet \"\"\"\n",
    "        model = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model.num_classes = num_classes\n",
    "        \n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet \"\"\"\n",
    "        model = models.densenet121(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3. Be careful, expects (299,299) sized images and has auxiliary output \"\"\"\n",
    "        model = models.inception_v3(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model.AuxLogits.fc.in_features\n",
    "        model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    # 체크포인트에서 모델 가중치를 로드할 경우\n",
    "    if checkpoint_path:\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3857f51c-40dd-49ca-9108-c893293f21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_fig(tensor):\n",
    "    # Move the tensor to CPU if it's on GPU\n",
    "    tensor = tensor.cpu()\n",
    "    # Convert the tensor to a numpy array\n",
    "    npimg = tensor.numpy()\n",
    "    # Transpose the dimensions from (C, H, W) to (H, W, C) to display with matplotlib\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "    # Display the image\n",
    "    ax.imshow(npimg)\n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "    return fig\n",
    "    \n",
    "# 훈련 함수\n",
    "def train_model(model, data_loader, device, criterion, optimizer, scheduler, num_epochs, writer=None):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    figure = None\n",
    "    for epoch in tqdm(range(num_epochs)):    \n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 각 에포크는 학습 단계와 검증 단계를 갖습니다.\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 학습 모드로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 평가 모드로 설정\n",
    "                figure_inputs = []\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 데이터를 배치 단위로 가져와 처리합니다.\n",
    "            for inputs, labels in data_loader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 파라미터 경사도를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 순전파\n",
    "                # 학습 시에만 연산 기록을 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 학습 단계인 경우 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 통계\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                if phase == 'valid' and figure is None:\n",
    "                    if len(figure_inputs) < 16:\n",
    "                        figure_inputs.append(inputs.cpu())  # Move inputs to CPU before appending\n",
    "                    if len(figure_inputs) >= 16:\n",
    "                        # Concatenate the list of inputs to form a batch\n",
    "                        figure_inputs = torch.cat(figure_inputs, dim=0)\n",
    "                        # Select only the first 16 images\n",
    "                        figure_inputs = figure_inputs[:16]\n",
    "                        # Convert the batch of images to a grid and then to a figure\n",
    "                        figure = tensor_to_fig(torchvision.utils.make_grid(figure_inputs, nrow=4))\n",
    "                        break\n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # TensorBoard에 로그 기록\n",
    "            if writer:\n",
    "                writer.add_scalar(f'{phase} Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar(f'{phase} Accuracy', epoch_acc, epoch)\n",
    "\n",
    "            if writer and phase == 'valid':\n",
    "                # 검증 단계에서만 로깅합니다.\n",
    "                if figure is not None:\n",
    "                    writer.add_figure(f'{phase} Figure', figure, epoch)\n",
    "                \n",
    "            # 모델을 깊은 복사(deep copy)함\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                plt.close(figure)  # Close the figure to prevent it from being displayed\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # 가장 나은 모델 가중치를 불러옴\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# 모델 준비\n",
    "#model_ft = initialize_model(model_name, num_classes=256, feature_extract=True, use_pretrained=True)\n",
    "#model_ft = model_ft.to(device)\n",
    "\n",
    "# 손실 함수 및 최적화 함수 설정\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#data_loader = {'train': train_data_loader, 'valid': valid_data_loader}\n",
    "#model_ft = train_model(model_ft, data_loader, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef0e291-5e03-464f-b00b-704682c6c670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\pytorch-hangul-recognition\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\GitHub\\pytorch-hangul-recognition\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 4.8937 Acc: 0.0812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 3.9250 Acc: 0.2004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 1/1 [02:20<00:00, 140.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 2m 21s\n",
      "Best val Acc: 0.200407\n",
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 4.8947 Acc: 0.0743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 3.9199 Acc: 0.1974\n",
      "\n",
      "Training complete in 0m 24s\n",
      "Best val Acc: 0.197355\n",
      "FOLD 2\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 4.8885 Acc: 0.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 3.8815 Acc: 0.1953\n",
      "\n",
      "Training complete in 0m 25s\n",
      "Best val Acc: 0.195320\n",
      "FOLD 3\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 4.8846 Acc: 0.0789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 3.8606 Acc: 0.2279\n",
      "\n",
      "Training complete in 0m 24s\n",
      "Best val Acc: 0.227874\n",
      "FOLD 4\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 4.8995 Acc: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:23<00:00, 23.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 3.8868 Acc: 0.2299\n",
      "\n",
      "Training complete in 0m 24s\n",
      "Best val Acc: 0.229908\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# k-fold 교차 검증 설정\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # PyTorch 데이터 로더에서 사용할 수 있도록 Subset 인덱스를 생성합니다.\n",
    "    train_subsampler = Subset(train_dataset, train_ids)\n",
    "    valid_subsampler = Subset(valid_dataset, valid_ids)\n",
    "    \n",
    "    # DataLoader를 생성합니다.\n",
    "    train_loader = DataLoader(train_subsampler, batch_size=64, shuffle=True, num_workers=4)\n",
    "    valid_loader = DataLoader(valid_subsampler, batch_size=64, shuffle=False, num_workers=4)\n",
    "    \n",
    "    data_loader = {'train': train_data_loader, 'valid': valid_data_loader}    \n",
    "\n",
    "    # 모델 초기화\n",
    "    model_ft = initialize_model(model_name, num_classes=256, feature_extract=True, use_pretrained=True)\n",
    "    model_ft = model_ft.to(device)\n",
    "        \n",
    "    # 손실 함수 및 최적화 함수 설정\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    \n",
    "    # 텐서보드 SummaryWriter 초기화\n",
    "    writer = SummaryWriter(f'runs/hangul_recognition_experiment_fold_{fold}')\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model_ft = train_model(model_ft, data_loader, device, criterion, optimizer_ft, exp_lr_scheduler, num_epochs, writer)\n",
    "    \n",
    "    # 모든 로깅을 완료했으면 writer를 닫습니다.\n",
    "    writer.close()\n",
    "    \n",
    "    # 결과 저장\n",
    "    results[fold] = model_ft\n",
    "    \n",
    "    # 모델 저장 (선택사항)\n",
    "    torch.save(model_ft.state_dict(), f'model_fold_{fold}.pth')\n",
    "\n",
    "print('K-FOLD CROSS VALIDATION RESULTS FOR {} FOLDS'.format(num_folds))\n",
    "print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765514d2-bcf7-493f-9bc2-b7e0a43c39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(results, device):\n",
    "    models_dict = {}\n",
    "    for fold, best_model_path in results.items():        \n",
    "        model_ft = initialize_model(model_name, num_classes=256, feature_extract=True, use_pretrained=False, checkpoint_path=best_model_path)                \n",
    "        model_ft = model_ft.to(device)\n",
    "        models_dict[fold] = model_ft\n",
    "    return models_dict\n",
    "\n",
    "def predict_model_kfold(models_dict, data_loader, device):\n",
    "    all_predictions = []\n",
    "    \n",
    "    for images, true_labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        # 각 이미지에 대해 모든 모델의 예측을 저장\n",
    "        predictions = []\n",
    "        for fold, model_ft in models_dict.items():\n",
    "            with torch.no_grad():\n",
    "                outputs = model_ft(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        # 다수결 투표로 최종 예측 결정\n",
    "        for i in range(len(images)):\n",
    "            image_predictions = [predictions[j] for j in range(i, len(predictions), len(images))]\n",
    "            final_pred = max(set(image_predictions), key=image_predictions.count)\n",
    "            all_predictions.append((final_pred, true_labels[i].item()))\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcbc2a11-1b35-4b09-a216-9c6a158767d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\pytorch-hangul-recognition\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    0: './model_fold_0.pth',\n",
    "    1: './model_fold_1.pth',\n",
    "    2: './model_fold_2.pth',\n",
    "    3: './model_fold_3.pth',\n",
    "    4: './model_fold_4.pth',\n",
    "}\n",
    "\n",
    "# 모든 모델 로드\n",
    "models_dict = initialize_models(results, device)\n",
    "\n",
    "# 검증 함수 호출\n",
    "predictions = predict_model_kfold(models_dict, test_data_loader, device)  # data_loader should be properly defined before calling\n",
    "\n",
    "# 결과 출력 및 정확도 계산\n",
    "correct = sum(1 for pred, true in predictions if pred == true)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff01f66-cbf2-4b47-b8e2-26827bac6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_at_index(label_file_path, index):\n",
    "    try:\n",
    "        with open(label_file_path, 'r', encoding='utf-8') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                if i == index:\n",
    "                    return line.strip()  # 공백 및 줄바꿈 제거\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {label_file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return None  # 파일을 찾지 못하거나 다른 에러 발생 시 None 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd44bd31-3e4e-4393-962b-401ef618032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_kfold(results, image_path, transform, device):    \n",
    "    # 각 모델의 예측 결과를 담을 리스트\n",
    "    predictions = []\n",
    "\n",
    "    # results 딕셔너리에서 각 fold의 state_dict을 로드하여 모델에 적용\n",
    "    for fold, state_dict in results.items():\n",
    "\n",
    "        # Load the saved state dict into the model\n",
    "        model_ft = initialize_model(model_name, num_classes=256, feature_extract=True, use_pretrained=False, checkpoint_path=f'./model_fold_{fold}.pth')        \n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "        # 예측 수행\n",
    "        predicted = predict_image(model_ft, image_path, transform, device)\n",
    "        predictions.append(predicted)\n",
    "\n",
    "\n",
    "    # 모든 모델의 예측 평균 계산\n",
    "    # 이 경우 다수결 투표가 더 적합할 수 있습니다 (가장 많이 예측된 클래스를 최종 결과로 선택)\n",
    "    for prediction in predictions:\n",
    "        print(get_label_at_index(label_file, prediction))\n",
    "    final_pred = max(set(predictions), key=predictions.count)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ceafa3-d9a6-41ba-9513-b227f8c8489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, transform, device):\n",
    "    # 이미지 불러오기\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    image = transform(image).unsqueeze(0) # 차원 추가 (배치 차원)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # 모델 추론 모드 설정 및 예측\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.item()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87f4831-c188-4d4e-9da9-d575f3d946f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiuq1/w9p+l+DPDmpxSSm/1FZHlRmG0KGwCBjIrlaKKKfFDLcSrFDG0kjHCqgyT+FW77RdU0uNJNQ027tEk+4Z4WTd9MiqNdh44YppnhC13ZCaJHIR6F5JD/LbXH0UUVa07UrzSL+K+sLh7e6iOUkTqpr2LWYm1/4b2eg/242oeJZ8aqYpXLmZcEeXGegIHO3vg14oQVYqwIIOCD2rpvHDY1HS7fPNvpVrEfb92D/WuYooooqW3uZrS5iubeV4pomDxupwVYHIINaXiLU7XWdQXUYLf7PcTxhruNfuGbozL6BvvY7EmrXjeUTeK7lgMAQ26DHosKD+lc9RRRRRRW14iMdw+nX0UiuLixhDgNkpJGvlMG9Cdm76MDWLRRRRRRRRRX//2Q==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날\n",
      "목\n",
      "없\n",
      "안\n",
      "발\n",
      "The predicted class for the image is: 발\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPyImage\n",
    "image_path = \"./image-data/hangul-images/hangul_111.jpeg\"\n",
    "display(IPyImage(filename=image_path))\n",
    "final_prediction = predict_image_kfold(results, image_path, transform, device)\n",
    "print(f\"The predicted class for the image is: {get_label_at_index(label_file, final_prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20eb40-53f5-4af0-806f-295156a0339f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
