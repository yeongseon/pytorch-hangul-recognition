{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d35898-dfc0-4002-86f8-b66b90dd5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3109145f-b9e2-489f-b0e8-eb83dff91ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 64 # 128\n",
    "NUM_WORKERS = 0 # 4\n",
    "MODEL_NAME = \"vit_b_32\"\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "# 장치 설정\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "data_csv_file = \"./image-data/labels-map.csv\"\n",
    "train_csv_file = \"./image-data/train-labels.csv\"\n",
    "valid_csv_file = \"./image-data/valid-labels.csv\"\n",
    "test_csv_file = \"./image-data/test-labels.csv\"\n",
    "label_file = \"./labels/256-common-hangul.txt\"\n",
    "#image_dir = \"./image-data/hangul-images\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "625b80be-0c61-4eda-b6b9-cb2c119b5edc",
   "metadata": {},
   "source": [
    "# Seed 설정 함수\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee0f3b0-3524-402d-8cfe-66ce8c783bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 122880\n",
      "Valid Dataset Size: 40960\n",
      "Test Dataset Size: 40960\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 분리 함수\n",
    "def split_dataset(csv_path, train_save_path, valid_save_path, test_save_path, test_size, valid_size, random_seed):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path, header=None, names=['path', 'label'])\n",
    "    \n",
    "    # 먼저 전체 데이터셋을 train + validation과 test로 분리\n",
    "    train_valid_df, test_df = train_test_split(df, test_size=test_size, random_state=random_seed, stratify=df['label'])\n",
    "    \n",
    "    # train + validation 데이터셋을 train과 validation으로 분리\n",
    "    train_size = 1 - valid_size\n",
    "    train_df, valid_df = train_test_split(train_valid_df, test_size=valid_size/(train_size + valid_size), random_state=random_seed, stratify=train_valid_df['label'])\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    train_df.to_csv(train_save_path, index=False, header=False)\n",
    "    valid_df.to_csv(valid_save_path, index=False, header=False)\n",
    "    test_df.to_csv(test_save_path, index=False, header=False)    \n",
    "\n",
    "# 데이터셋 클래스\n",
    "class KoreanHandwritingDataset(Dataset):\n",
    "    def __init__(self, csv_file, label_file, transform=None):\n",
    "        self.dataset = pd.read_csv(csv_file, header=None, names=['path', 'label'])\n",
    "        #self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "        with open(self.label_file, 'r', encoding='utf-8') as f:\n",
    "            hangul_chars = [line.strip() for line in f.readlines()]\n",
    "        self.label_mapping = {char: idx for idx, char in enumerate(hangul_chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #img_path = os.path.join(self.image_dir, os.path.basename(self.dataset.iloc[idx]['path']))\n",
    "        \n",
    "        # 그레이스케일로 이미지 열기\n",
    "        img_path = self.dataset.iloc[idx]['path']\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "        label = self.label_mapping[label]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "      \n",
    "            \n",
    "# 데이터 변환\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 먼저 이미지를 더 크게 리사이즈\n",
    "    transforms.CenterCrop(224),  # 그 다음 중앙을 224x224 크기로 크롭\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "# 기존의 데이터 변환 파이프라인에 데이터 증강을 추가합니다.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 이미지를 256x256으로 리사이즈\n",
    "    transforms.RandomResizedCrop(224),  # 랜덤한 위치에서 크롭 후 224x224로 리사이즈\n",
    "    transforms.RandomRotation(degrees=15),  # 랜덤하게 이미지 회전\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 수평 뒤집기\n",
    "    #transforms.RandomVerticalFlip(p=0.5),  # 50% 확률로 수직 뒤집기\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 색상 조정\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),  # 아핀 변환\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),  # 투시 변환\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),  # 가우시안 블러 적용\n",
    "    transforms.Grayscale(num_output_channels=3),  # 흑백 이미지로 변환\n",
    "    transforms.ToTensor(),  # 이미지를 텐서로 변환\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 정규화\n",
    "])\n",
    "\"\"\"\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 먼저 이미지를 더 크게 리사이즈\n",
    "    transforms.CenterCrop(224),  # 그 다음 중앙을 224x224 크기로 크롭\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # 먼저 이미지를 더 크게 리사이즈\n",
    "    transforms.CenterCrop(224),  # 그 다음 중앙을 224x224 크기로 크롭\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# 데이터셋 및 데이터 로더 준비\n",
    "# split_dataset(data_csv_file, train_csv_file, valid_csv_file, test_csv_file, 0.2, 0.1, 42)\n",
    "split_dataset(data_csv_file, train_csv_file, valid_csv_file, test_csv_file, 0.2, 0.25, 42) # 60 / 20 / 20\n",
    "\n",
    "train_dataset = KoreanHandwritingDataset(train_csv_file, label_file, train_transform)\n",
    "valid_dataset = KoreanHandwritingDataset(valid_csv_file, label_file, valid_transform)\n",
    "test_dataset = KoreanHandwritingDataset(test_csv_file, label_file, test_transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Valid Dataset Size: {len(valid_dataset)}\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa2351d-2086-4516-bfdf-c2404534244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True, checkpoint_path=None):\n",
    "    # 모델을 선택하고 초기화합니다.\n",
    "    if model_name == \"resnet101\":\n",
    "        \"\"\" Resnet101 \"\"\"\n",
    "        model = models.resnet101(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet \"\"\"\n",
    "        model = models.alexnet(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        \n",
    "    elif model_name == \"mobilenet\":\n",
    "        \"\"\" Mobilenet_v2 \"\"\"\n",
    "        model = models.mobilenet_v2(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"vgg19\":\n",
    "        \"\"\" VGG19 \"\"\"\n",
    "        model = models.vgg19(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"vgg19_bn\":\n",
    "        \"\"\" VGG19_bn \"\"\"\n",
    "        model = models.vgg19_bn(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet \"\"\"\n",
    "        model = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model.num_classes = num_classes\n",
    "        \n",
    "    elif model_name == \"densenet121\":\n",
    "        \"\"\" Densenet \"\"\"\n",
    "        model = models.densenet121(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3. Be careful, expects (299,299) sized images and has auxiliary output \"\"\"\n",
    "        model = models.inception_v3(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model.AuxLogits.fc.in_features\n",
    "        model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    elif model_name == \"vit_b_16\":\n",
    "        \"\"\" Vision Transformer B-16 \"\"\"\n",
    "        model = models.vit_b_16(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # ViT의 분류기 부분을 교체합니다.\n",
    "        num_ftrs = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"vit_b_32\":\n",
    "        \"\"\" Vision Transformer B-32 \"\"\"\n",
    "        model = models.vit_b_32(pretrained=use_pretrained)\n",
    "        if feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # ViT의 분류기 부분을 교체합니다.\n",
    "        num_ftrs = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    # 체크포인트에서 모델 가중치를 로드할 경우\n",
    "    if checkpoint_path:\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3857f51c-40dd-49ca-9108-c893293f21e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\aistage-baseline\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\GitHub\\aistage-baseline\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_32_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_32_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vit_b_32-d86f8d99.pth\" to C:\\Users\\YeongseonChoe/.cache\\torch\\hub\\checkpoints\\vit_b_32-d86f8d99.pth\n",
      "100%|██████████████████████████████████████████████████████| 337M/337M [00:17<00:00, 19.9MB/s]\n",
      "  0%|                                                                  | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.7845 Acc: 0.6194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      " 10%|█████▍                                                | 1/10 [45:30<6:49:30, 2730.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.0235 Acc: 0.0200\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.8095 Acc: 0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████▍                                         | 2/10 [1:14:17<4:45:21, 2140.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8181 Acc: 0.8425\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.7644 Acc: 0.8622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████▌                                    | 3/10 [1:30:30<3:07:30, 1607.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8050 Acc: 0.8474\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.7585 Acc: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████▊                               | 4/10 [1:47:12<2:16:49, 1368.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8043 Acc: 0.8478\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.7579 Acc: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████                          | 5/10 [2:03:42<1:42:40, 1232.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8042 Acc: 0.8478\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.7579 Acc: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████▏                    | 6/10 [2:20:21<1:16:51, 1152.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8042 Acc: 0.8477\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.7579 Acc: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████▊                | 7/10 [2:36:19<54:26, 1088.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8042 Acc: 0.8477\n",
      "Early stopping\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.7579 Acc: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████▏          | 8/10 [2:52:22<34:57, 1048.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8042 Acc: 0.8477\n",
      "Early stopping\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.7579 Acc: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████▌     | 9/10 [3:08:25<17:02, 1022.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8042 Acc: 0.8477\n",
      "Early stopping\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.7579 Acc: 0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 10/10 [3:35:00<00:00, 1290.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.8042 Acc: 0.8477\n",
      "Early stopping\n",
      "\n",
      "Training complete in 215m 0s\n",
      "Best val Acc: 0.847803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tensor_to_fig(tensor):\n",
    "    # Move the tensor to CPU if it's on GPU\n",
    "    tensor = tensor.cpu()\n",
    "    # Convert the tensor to a numpy array\n",
    "    npimg = tensor.numpy()\n",
    "    # Transpose the dimensions from (C, H, W) to (H, W, C) to display with matplotlib\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "    # Display the image\n",
    "    ax.imshow(npimg)\n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# 훈련 함수\n",
    "def train_model(model, data_loader, device, criterion, optimizer, scheduler, num_epochs, writer=None):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    figure = None\n",
    "\n",
    "    # Early Stopping을 위한 변수 초기화\n",
    "    patience = 5\n",
    "    no_improvement_count = 0\n",
    "    early_stop = False\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):    \n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 각 에포크는 학습 단계와 검증 단계를 갖습니다.\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 학습 모드로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 평가 모드로 설정\n",
    "                figure_inputs = []\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 데이터를 배치 단위로 가져와 처리합니다.\n",
    "            for inputs, labels in data_loader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 파라미터 경사도를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 순전파\n",
    "                # 학습 시에만 연산 기록을 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 학습 단계인 경우 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 통계\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                if phase == 'valid' and figure is None:\n",
    "                    if len(figure_inputs) < 16:\n",
    "                        figure_inputs.append(inputs.cpu())  # Move inputs to CPU before appending\n",
    "                    if len(figure_inputs) >= 16:\n",
    "                        # Concatenate the list of inputs to form a batch\n",
    "                        figure_inputs = torch.cat(figure_inputs, dim=0)\n",
    "                        # Select only the first 16 images\n",
    "                        figure_inputs = figure_inputs[:16]\n",
    "                        # Convert the batch of images to a grid and then to a figure\n",
    "                        figure = tensor_to_fig(torchvision.utils.make_grid(figure_inputs, nrow=4))\n",
    "                        break\n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # TensorBoard에 로그 기록\n",
    "            if writer:\n",
    "                writer.add_scalar(f'{phase} Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar(f'{phase} Accuracy', epoch_acc, epoch)\n",
    "\n",
    "            if writer and phase == 'valid':\n",
    "                # 검증 단계에서만 로깅합니다.\n",
    "                if figure is not None:\n",
    "                    writer.add_figure(f'{phase} Figure', figure, epoch)\n",
    "                    \n",
    "            if phase == 'valid':\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    no_improvement_count = 0\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "\n",
    "                if no_improvement_count > patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "                    \n",
    "            # 모델을 깊은 복사(deep copy)함\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                plt.close(figure)  # Close the figure to prevent it from being displayed\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # 가장 나은 모델 가중치를 불러옴\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# 모델 준비\n",
    "model_ft = initialize_model(MODEL_NAME, num_classes=256, feature_extract=True, use_pretrained=True)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "\n",
    "# 손실 함수 및 최적화 함수 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer 및 Learning Rate Scheduler 설정\n",
    "# optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-08)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)  # Decay LR by a factor of 0.1 every epoch\n",
    "\n",
    "data_loader = {'train': train_data_loader, 'valid': valid_data_loader}\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "path = f'logs/{MODEL_NAME}_{current_time}'\n",
    "writer = SummaryWriter(path)\n",
    "model_ft = train_model(model_ft, data_loader, DEVICE, criterion, optimizer_ft, exp_lr_scheduler, NUM_EPOCHS, writer)\n",
    "writer.close()\n",
    "torch.save(model_ft.state_dict(), f'{path}/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20e6c1-689e-4d10-b714-87af75d7f223",
   "metadata": {},
   "source": [
    "# k-fold 교차 검증 설정\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # PyTorch 데이터 로더에서 사용할 수 있도록 Subset 인덱스를 생성합니다.\n",
    "    train_subsampler = Subset(train_dataset, train_ids)\n",
    "    valid_subsampler = Subset(valid_dataset, valid_ids)\n",
    "    \n",
    "    # DataLoader를 생성합니다.\n",
    "    train_loader = DataLoader(train_subsampler, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    valid_loader = DataLoader(valid_subsampler, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    data_loader = {'train': train_data_loader, 'valid': valid_data_loader}    \n",
    "\n",
    "    # 모델 초기화\n",
    "    model_ft = initialize_model(MODEL_NAME, num_classes=256, feature_extract=True, use_pretrained=True)\n",
    "    model_ft = model_ft.to(DEVICE)\n",
    "        \n",
    "    # 손실 함수 및 최적화 함수 설정\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1) # 3번 epoch마다 lr이 0.1씩 감소\n",
    "    \n",
    "    # 텐서보드 SummaryWriter 초기화\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    path = f'logs/{MODEL_NAME}/fold_{fold}_{current_time}'\n",
    "\n",
    "\n",
    "    writer = SummaryWritepath}')\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model_ft = train_model(model_ft, data_loader, DEVICE, criterion, optimizer_ft, exp_lr_scheduler, num_epochs, writer)\n",
    "    \n",
    "    # 모든 로깅을 완료했으면 writer를 닫습니다.\n",
    "    writer.close()\n",
    "    \n",
    "    # 결과 저장\n",
    "    results[fold] = model_ft\n",
    "    \n",
    "    # 모델 저장 (선택사항)\n",
    "    torch.save(model_ft.state_dict(), f'model_fold_{fold}.pth')\n",
    "\n",
    "print('K-FOLD CROSS VALIDATION RESULTS FOR {} FOLDS'.format(num_folds))\n",
    "print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765514d2-bcf7-493f-9bc2-b7e0a43c39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(results, device):\n",
    "    models_dict = {}\n",
    "    for fold, best_model_path in results.items():        \n",
    "        model_ft = initialize_model(model_name, num_classes=256, feature_extract=True, use_pretrained=False, checkpoint_path=best_model_path)                \n",
    "        model_ft = model_ft.to(device)\n",
    "        models_dict[fold] = model_ft\n",
    "    return models_dict\n",
    "\n",
    "def predict_model_kfold(models_dict, data_loader, device):\n",
    "    all_predictions = []\n",
    "    \n",
    "    for images, true_labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        # 각 이미지에 대해 모든 모델의 예측을 저장\n",
    "        predictions = []\n",
    "        for fold, model_ft in models_dict.items():\n",
    "            with torch.no_grad():\n",
    "                outputs = model_ft(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        # 다수결 투표로 최종 예측 결정\n",
    "        for i in range(len(images)):\n",
    "            image_predictions = [predictions[j] for j in range(i, len(predictions), len(images))]\n",
    "            final_pred = max(set(image_predictions), key=image_predictions.count)\n",
    "            all_predictions.append((final_pred, true_labels[i].item()))\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2868578-6653-4b3d-9646-7b8819b878c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "def predict_single_model(model, data_loader, device):\n",
    "    all_predictions = []\n",
    "    \n",
    "    for images, true_labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                all_predictions.append((predicted[i].item(), true_labels[i].item()))\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# 검증 함수 호출\n",
    "predictions = predict_single_model(model_ft, test_data_loader, DEVICE)\n",
    "\n",
    "# 결과 출력 및 정확도 계산\n",
    "correct = sum(1 for pred, true in predictions if pred == true)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b053d-efea-46ed-99e9-f048abc08044",
   "metadata": {},
   "source": [
    "results = {\n",
    "    0: './model_fold_0.pth',\n",
    "    1: './model_fold_1.pth',\n",
    "    2: './model_fold_2.pth',\n",
    "    3: './model_fold_3.pth',\n",
    "    4: './model_fold_4.pth',\n",
    "}\n",
    "\n",
    "# 모든 모델 로드\n",
    "models_dict = initialize_models(results, DEVICE)\n",
    "\n",
    "# 검증 함수 호출\n",
    "predictions = predict_model_kfold(models_dict, test_data_loader, DEVICE)  # data_loader should be properly defined before calling\n",
    "\n",
    "# 결과 출력 및 정확도 계산\n",
    "correct = sum(1 for pred, true in predictions if pred == true)\n",
    "accuracy = correct / len(predictions)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff01f66-cbf2-4b47-b8e2-26827bac6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_at_index(label_file_path, index):\n",
    "    try:\n",
    "        with open(label_file_path, 'r', encoding='utf-8') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                if i == index:\n",
    "                    return line.strip()  # 공백 및 줄바꿈 제거\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {label_file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return None  # 파일을 찾지 못하거나 다른 에러 발생 시 None 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd44bd31-3e4e-4393-962b-401ef618032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_kfold(results, image_path, transform, device):    \n",
    "    # 각 모델의 예측 결과를 담을 리스트\n",
    "    predictions = []\n",
    "\n",
    "    # results 딕셔너리에서 각 fold의 state_dict을 로드하여 모델에 적용\n",
    "    for fold, state_dict in results.items():\n",
    "\n",
    "        # Load the saved state dict into the model\n",
    "        model_ft = initialize_model(model_name, num_classes=256, feature_extract=True, use_pretrained=False, checkpoint_path=f'./model_fold_{fold}.pth')        \n",
    "        model_ft = model_ft.to(device)\n",
    "\n",
    "        # 예측 수행\n",
    "        predicted = predict_image(model_ft, image_path, transform, device)\n",
    "        predictions.append(predicted)\n",
    "\n",
    "\n",
    "    # 모든 모델의 예측 평균 계산\n",
    "    # 이 경우 다수결 투표가 더 적합할 수 있습니다 (가장 많이 예측된 클래스를 최종 결과로 선택)\n",
    "    for prediction in predictions:\n",
    "        print(get_label_at_index(label_file, prediction))\n",
    "    final_pred = max(set(predictions), key=predictions.count)\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ceafa3-d9a6-41ba-9513-b227f8c8489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, transform, device):\n",
    "    # 이미지 불러오기\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    image = transform(image).unsqueeze(0) # 차원 추가 (배치 차원)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # 모델 추론 모드 설정 및 예측\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.item()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87f4831-c188-4d4e-9da9-d575f3d946f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiul8L+C7vxNDc3n22y0/TrVgs11dzBVBPIAHVj7V1Gp/D3w1a/D+/wDEGn+I5L+a1lWEbYdsbOSOBnnpmvMgCSAOprpfHnh638L+KJNLtmdo0ghcs5ySzxqx/U1zVFFFFe7ad4MOq/A610OzuI01y6dtVitmbDXCDjaM+2PxHvXhrxyW1w0csbRyxPtdHGCpB5BHY16J8Xohc3vh/XYzmPUtJhZm/wCmiDaw/DArzeiiiiux1z4g3t/rmj6lpqNp76TbpDbhX3Y29SeOhOeKo+NPFKeMNZGqnTLeyunQC4MBOJWH8RB6GtPxHrVpqXwy8IWInR76we7SVAfmVGcFc/0riaKKKKKKKKKKKKKKKKK//9k=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for the image is: 지\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPyImage\n",
    "image_path = \"./image-data/font2/hangul-images/hangul_106.jpeg\"\n",
    "display(IPyImage(filename=image_path))\n",
    "#final_prediction = predict_image_kfold(results, image_path, transform, DEVICE)\n",
    "final_prediction = predict_image(model_ft, image_path, transform, DEVICE)\n",
    "print(f\"The predicted class for the image is: {get_label_at_index(label_file, final_prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20eb40-53f5-4af0-806f-295156a0339f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
